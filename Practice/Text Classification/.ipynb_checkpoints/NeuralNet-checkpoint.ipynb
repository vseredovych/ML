{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(cls, x_1, x_2, ax=None, threshold=0.5, contourf=False):\n",
    "    xx1, xx2 = np.meshgrid(np.linspace(x_1.min(), x_1.max(), 100), \n",
    "                           np.linspace(x_2.min(), x_2.max(), 100))\n",
    "\n",
    "    X_pred = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "    pred = cls.predict_proba(X_pred)[:, 0]\n",
    "    Z = pred.reshape((100, 100))\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.contour(xx1, xx2, Z, levels=[threshold], colors='black')\n",
    "    ax.set_xlim((x_1.min(), x_1.max()))\n",
    "    ax.set_ylim((x_2.min(), x_2.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_data(X, y):\n",
    "#     ax = plt.gca()\n",
    "#     ax.scatter(X[:,0], X[:,1], c=(y == 1), cmap=cm_bright)\n",
    "    \n",
    "def plot_data(A, b, test = False):\n",
    "    positive_indices = np.where(b == 1)[0]\n",
    "    negative_indices = np.where(b == 0)[0]\n",
    "    \n",
    "    plt.scatter(A[positive_indices, 0], A[positive_indices, 1], marker='x', c= 'yellow' if test else 'green')\n",
    "    plt.scatter(A[negative_indices, 0], A[negative_indices, 1], marker='+', c= 'blue' if test else 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    NN for binary classification\n",
    "    Attributes:\n",
    "    ...\n",
    "    \"\"\"\n",
    "    # hidden_layer_sizes=(100, 50,)\n",
    "    def __init__(self, hidden_layer_sizes, normalize = True, learning_rate = 0.01, num_iter = 30000, optimization='', momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalize = normalize\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.optimization = optimization\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        \"\"\"\n",
    "        Зверніть увагу, що нормалізація вхідних даних є дуже важливою для швидкодії нейронних мереж.\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "    \n",
    "        if mean is None:   \n",
    "            mean = np.zeros([n, 1])\n",
    "        if std is None:\n",
    "            std  = np.ones([n, 1])\n",
    "        \n",
    "        for i in range(n):\n",
    "            if (np.std(X[:, i]) != 0):\n",
    "                if mean is None:\n",
    "                    mean[i] = np.mean(X[:, i])\n",
    "                if std is None:\n",
    "                    std[i] = np.std(X[:, i])\n",
    "        \n",
    "        X_new = (X - mean) / std\n",
    "        return X_new, mean, std\n",
    "    \n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        В наступних практичних потрібно буде додати підтримку й інших активаційних функцій - це один з гіперпараметрів. \n",
    "        Їх можна вибирати для всіх шарів одночасно або мати різні активаційні функції на кожному з них.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __sigmoid_derivative(self, Z):\n",
    "        z = self.__sigmoid(Z)\n",
    "        return np.multiply(z, (1 - z))\n",
    "    \n",
    "    def __softmax(self, Z):\n",
    "        exp_z = np.exp(Z)\n",
    "        return exp_z / exp_z.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    def __cross_entropy(self, A, Y):\n",
    "        return - np.sum(Y * np.log(A), axis=1)\n",
    "    \n",
    "    def __initialize_parameters(self, n_x, n_y):\n",
    "        self.parameters = {}\n",
    "        n = len(n_x)\n",
    "        \n",
    "        for i in range(1, len(n_x)):\n",
    "            W = np.random.randn(n_x[i], n_x[i - 1]) * 0.01\n",
    "            b = np.zeros((n_x[i], 1))\n",
    "            self.parameters.update({f\"W{i}\": W, f\"b{i}\": b})\n",
    "\n",
    "        W = np.random.randn(n_y, n_x[n-1]) * 0.01\n",
    "        b = np.zeros((n_y, 1))\n",
    "        \n",
    "        self.parameters.update({f\"W{n}\":W, f\"b{n}\":b})\n",
    "                    \n",
    "        for i in range(1, len(n_x) + 1):\n",
    "            self.parameters.update({f\"VdW{i}\": 0, f\"Vdb{i}\": 0})\n",
    "            \n",
    "    def __forward_propagation(self, X):\n",
    "        num_layers = len(self.hidden_layer_sizes)\n",
    "        cache = self.parameters.copy()\n",
    "        \n",
    "        A = X\n",
    "        for i in range(1, num_layers + 2):\n",
    "            if i == num_layers + 1:\n",
    "                W = self.parameters[f\"W{i}\"]\n",
    "                b = self.parameters[f\"b{i}\"]\n",
    "                Z = np.dot(W, A) + b\n",
    "                A = self.__softmax(Z)\n",
    "            else:            \n",
    "                W = self.parameters[f\"W{i}\"]\n",
    "                b = self.parameters[f\"b{i}\"]\n",
    "                Z = np.dot(W, A) + b\n",
    "                A = self.__sigmoid(Z)\n",
    "            cache.update({f\"Z{i}\": Z})\n",
    "            cache.update({f\"A{i}\": A})\n",
    "        return A, cache\n",
    "\n",
    "    def compute_cost(self, A, Y):\n",
    "        J = -np.mean(Y.T * np.log(A.T))\n",
    "        return J\n",
    "    \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        n = X.shape[0]\n",
    "        num_layers = len(self.hidden_layer_sizes)\n",
    "        \n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(num_layers+1, 0, -1):\n",
    "            W = cache[f\"W{i}\"]\n",
    "            b = cache[f\"b{i}\"]\n",
    "            Z = cache[f\"Z{i}\"]\n",
    "            if i == num_layers + 1:\n",
    "                A = cache[f\"A{i}\"]\n",
    "                A_next = cache[f\"A{i - 1}\"]\n",
    "                \n",
    "                dZ = A - Y\n",
    "                dW = 1. / m * np.dot(dZ, A_next.T)\n",
    "                db = 1. / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "            elif (i == 1):\n",
    "                W_prev = cache[f\"W{i + 1}\"]\n",
    "                A = cache[f\"A{i}\"]\n",
    "                \n",
    "                dA = np.dot(W_prev.T, dZ)\n",
    "                dZ = np.multiply(dA, self.__sigmoid_derivative(A))\n",
    "                dW = 1. / m * np.dot(dZ, X.T)\n",
    "                db = 1. / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "            else:\n",
    "                W_prev = cache[f\"W{i + 1}\"]\n",
    "                A = cache[f\"A{i}\"]\n",
    "                A_next = cache[f\"A{i - 1}\"]\n",
    "\n",
    "                dA = np.dot(W_prev.T, dZ)\n",
    "                dZ = np.multiply(dA, self.__sigmoid_derivative(A))\n",
    "                dW = 1. / m * np.dot(dZ, A_next.T)\n",
    "                db = 1. / m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "            grads.update({f\"dZ{i}\":dZ, f\"dW{i}\":dW, f\"db{i}\":db})\n",
    "        return grads\n",
    "    \n",
    "    def __update_parameters(self, grads, iteration):\n",
    "        num_layers = len(self.hidden_layer_sizes)\n",
    "        \n",
    "        for i in range(1, num_layers + 2):\n",
    "            W = self.parameters[f\"W{i}\"]\n",
    "            b = self.parameters[f\"b{i}\"]\n",
    "            \n",
    "            dW = grads[f\"dW{i}\"]\n",
    "            db = grads[f\"db{i}\"]\n",
    "            if self.optimization == 'momentum':\n",
    "#                 print(1 - self.momentum ** iteration)\n",
    "                self.parameters[f\"VdW{i}\"] = (self.momentum * self.parameters[f\"VdW{i}\"] + (1 - self.momentum) * dW / (1 - self.momentum ** (iteration + 1)))\n",
    "                self.parameters[f\"Vdb{i}\"] = (self.momentum * self.parameters[f\"Vdb{i}\"] + (1 - self.momentum) * db / (1 - self.momentum ** (iteration + 1)))\n",
    "\n",
    "                self.parameters[f\"W{i}\"] = W - self.learning_rate * self.parameters[f\"VdW{i}\"]\n",
    "                self.parameters[f\"b{i}\"] = b - self.learning_rate * self.parameters[f\"Vdb{i}\"]\n",
    "\n",
    "            else:\n",
    "                self.parameters[f\"W{i}\"] = W - self.learning_rate * dW\n",
    "                self.parameters[f\"b{i}\"] = b - self.learning_rate * db\n",
    "            \n",
    "    def fit(self, X_vert, Y_vert, epsilon=1e-08, print_cost = True):\n",
    "        \n",
    "        X, Y = X_vert.T, Y_vert.T\n",
    "        \n",
    "        if self.normalize:\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "        \n",
    "        costs = []\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        n_x = (X.shape[0],) + self.hidden_layer_sizes\n",
    "        n_y = Y.shape[0]\n",
    "        \n",
    "        self.__initialize_parameters(n_x, n_y)\n",
    "        \n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            grads = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(grads, i)\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "                if i > 1:\n",
    "                    print(f\"Delta: {costs[-2] - costs[-1]}\")\n",
    "       \n",
    "            if i > 1 and abs(costs[-2] - costs[-1]) < epsilon:\n",
    "                break\n",
    "                \n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(\"Iteration, *1000\")\n",
    "            plt.show()\n",
    "\n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)\n",
    "        \n",
    "        probs = self.__forward_propagation(X)[0]\n",
    "        return probs.T\n",
    "    \n",
    "    def predict(self, X_vert):\n",
    "        positive_probs = self.predict_proba(X_vert)\n",
    "        y_pred = self.likehood_func(positive_probs)\n",
    "        return y_pred  \n",
    "\n",
    "    def likehood_func(self, z):\n",
    "        return z.argmax(axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit72af3e13b1864d0a85758af773701f9c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
